{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Machine Learning (Classification))\n",
    "## 1.Part a)\n",
    "Choose one of the toy classification datasets bundled with sklearn other than the digits dataset. \n",
    "\n",
    "## Answer:\n",
    "\n",
    "\"Breast Cancer data-set\" is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Part b)\n",
    "\n",
    "Train three distinct sklearn classification estimators for the chosen dataset and compare the results to see which one performs the best when using 2-fold cross-validation. Note that you should use three distinct classification models here (not just tweak underlying parameters). A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use classifiers! Unless you have an inclination to do otherwise, I recommend using the model default parameters when available. \n",
    "\n",
    "## Answer:\n",
    "\n",
    "For \"Classification & Train sklearn\" I chose \"K-NN, SVC & GaussianNB\" with \"2-fold cross-validation\" methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4a1765ec984b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Clasicication with 2-fold cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m estimators = {\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;34m'KNeighborsClassifier'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;34m'SVC'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     'GaussianNB': GaussianNB()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'knn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#reading data set\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "#Clasicication with 2-fold cross-validation\n",
    "estimators = {\n",
    "    'KNeighborsClassifier': knn, \n",
    "    'SVC': SVC(),\n",
    "    'GaussianNB': GaussianNB()}\n",
    "\n",
    "twoFoldAccuracy=[]\n",
    "twoFoldSd=[]\n",
    "for estimator_name, estimator_object in estimators.items():\n",
    "    kfold = KFold(n_splits=2, random_state=11, shuffle=True)\n",
    "    scores = cross_val_score(estimator=estimator_object, \n",
    "        X=cancer.data, y=cancer.target, cv=kfold)\n",
    "    print(f'{estimator_name:>20}: ' + \n",
    "          f'mean accuracy={scores.mean():.2%}; ' +\n",
    "          f'standard deviation={scores.std():.2%}')\n",
    "    twoFoldAccuracy.append(scores.mean())\n",
    "    twoFoldSd.append(scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Part c) \n",
    "Repeat a. for 20-fold cross-validation. Explain in a paragraph the difference in your results when using 20-fold vs 2-fold cross-validation (if any). \n",
    "\n",
    "## Answer:\n",
    "\n",
    "As can be seen from the results in the tables, the accuracy of \"GaussianNB\" is the best one from three applied classification methods for comparison between 2-fold and 20-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier2: mean accuracy=92.80%; standard deviation=4.48%\n",
      "                SVC2: mean accuracy=91.39%; standard deviation=6.40%\n",
      "         GaussianNB2: mean accuracy=93.85%; standard deviation=3.96%\n",
      "\n",
      "         \u001b[1m2-FOLD CROSS-VALIDATION\u001b[0m           \n",
      "             Mean Accuracy  STD deviation\n",
      "KNN              0.933222       0.003398\n",
      "SVC              0.912132       0.003361\n",
      "GaussianNB       0.940246       0.000105\n",
      "\n",
      "          \u001b[1m20-FOLD CROSS-VALIDATION\u001b[0m          \n",
      "             Mean Accuracy  STD deviation\n",
      "KNN              0.928017       0.044761\n",
      "SVC              0.913855       0.063953\n",
      "GaussianNB       0.938547       0.039608\n"
     ]
    }
   ],
   "source": [
    "#Clasicication with 20-fold cross-validation\n",
    "estimators = {\n",
    "    'KNeighborsClassifier2': knn, \n",
    "    'SVC2': SVC(),\n",
    "    'GaussianNB2': GaussianNB()}\n",
    "\n",
    "twentyFoldAccuracy=[]\n",
    "twentyFoldSd=[]\n",
    "for estimator_name, estimator_object in estimators.items():\n",
    "    kfold2 = KFold(n_splits=20, random_state=11, shuffle=True)\n",
    "    scores2 = cross_val_score(estimator=estimator_object, \n",
    "        X=cancer.data, y=cancer.target, cv=kfold2)\n",
    "    print(f'{estimator_name:>20}: ' + \n",
    "          f'mean accuracy={scores2.mean():.2%}; ' +\n",
    "          f'standard deviation={scores2.std():.2%}')\n",
    "    twentyFoldAccuracy.append(scores2.mean())\n",
    "    twentyFoldSd.append(scores2.std())\n",
    "# Choose the estimator with the highest mean accuracy score & lower std deviation\n",
    "print()\n",
    "import pandas as pd\n",
    "final_results2 = {'Mean Accuracy': twoFoldAccuracy, 'STD deviation': twoFoldSd}\n",
    "final_results20 = {'Mean Accuracy': twentyFoldAccuracy, 'STD deviation': twentyFoldSd}\n",
    "fr2=pd.DataFrame(final_results2)\n",
    "fr2.index = ['KNN', 'SVC', 'GaussianNB']\n",
    "fr20=pd.DataFrame(final_results20)\n",
    "fr20.index = ['KNN', 'SVC', 'GaussianNB']\n",
    "title = \"\\033[1m2-FOLD CROSS-VALIDATION\\033[0m\"\n",
    "print(title.center(50),\"\\n\",fr2)\n",
    "title = \"\\033[1m20-FOLD CROSS-VALIDATION\\033[0m\"\n",
    "print(\"\\n\",title.center(50),\"\\n\",fr20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Part d) \n",
    "Construct a confusion matrix for your most accurate model between the three estimators and two cross-fold options\n",
    "\n",
    "## Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier incorrectly predicted 7 of 143 samples.\n",
      "The Accuracy is= 95.10%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "gnb = GaussianNB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=11)\n",
    "gnb.fit(X=X_train, y=y_train)\n",
    "predicted_labels = gnb.predict(X=X_test)\n",
    "expected_labels = y_test\n",
    "wrong = [(p, e) for (p, e) in zip(predicted_labels, expected_labels) if p != e]\n",
    "print(f'The classifier incorrectly predicted {len(wrong)} of {len(expected_labels)} samples.')\n",
    "print('The Accuracy is=',f'{gnb.score(X_test, y_test):.2%}')\n",
    "confusion = confusion_matrix(y_true=expected_labels, y_pred=predicted_labels)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "confusion_fr = pd.DataFrame(confusion, index=range(2), columns=range(2))\n",
    "figure = plt.figure(figsize=(5, 4))\n",
    "axes = sns.heatmap(confusion_fr, annot=True, \n",
    "                   cmap='nipy_spectral_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Part e) \n",
    "Which class in your dataset is most accurately predicted to have the correct label by the best classifier, and which is most likely to be confused among one or more of the wrong classes?(You can use a cell in a jupyter notebook file for this or a separate text/document file).\n",
    "\n",
    "## Answer:\n",
    "The confusion matrix verifies that we have 44 True Positives (44 True Detection from Class 0), 6 False Positives (6 False Detection from Class 0), 1 False Negatives (1 False Detection from Class 1) and 92 False Negatives(92 True Detection from Class 1).\n",
    "\n",
    "Then, the best class will class 1 with 92 True Detection (98.93%) of predited data and 1 Flase Detecteion (1.07%). \n",
    "\n",
    "And, class 0 with 6 Flase Detection (12%) of predicted data and 44 True Detection (88%) is most confused class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Option I). (Machine Learning (Regression))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part a)\n",
    "Locate a non-proprietary, small-scale dataset suitable for regression online. There are countless sources and repositories than you can use in this task, but if you have trouble finding one, I recommend starting via Kaggle (https://www.kaggle.com/code/rtatman/datasets-for-regression-analysis/notebook). Explain briefly what the dataset represents, what target variable you will be using, and what other features are present. You may want or need to apply preprocessing to your data to insure it can be used properly with the regression models (e.g. making every feature numeric through transformation or by dropping some)\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Question I used Admission Dataset in the universities available on: https://www.kaggle.com/datasets/akshaydattatraykhare/data-for-admission-in-the-university\n",
    "\n",
    "This dataset includes various information like GRE score, TOEFL score, university rating, SOP (Statement of Purpose), LOR (Letter of Recommendation), CGPA, research and chance of admit. In this dataset, 400 entries are included.\n",
    "\n",
    "GRE Scores ( out of 340 )\n",
    "TOEFL Scores ( out of 120 )\n",
    "University Rating ( out of 5 )\n",
    "Statement of Purpose (SOP) and Letter of Recommendation (LOR) Strength ( out of 5 )\n",
    "Undergraduate GPA ( out of 10 )\n",
    "Research Experience ( either 0 or 1 )\n",
    "Chance of Admit ( ranging from 0 to 1 )\n",
    "\n",
    "- The target variable is the \"Chance of Admit\".\n",
    "\n",
    "- Evaluation of the dataset shows that all data are numerical and there are no missing data or noisy inputs. Therefore, we skip the preprocessing step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Part b)\n",
    "Train three distinct sklearn regression estimators for the chosen dataset and compare the results to see which one performs the best when using 10-fold cross-validation, utilizing the R-Squared score to gauge performance. Note that you should use two distinct regression models here (not just tweak underlying parameters). A relatively complete listing of the available estimators can be found here (https://scikit-learn.org/stable/supervised_learning.html) -- but make sure you only use regression models! Unless you have an inclination to do otherwise, I recommend using the model default parameters when available.\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean R-Squared Score: 0.7807105665823387\n",
      "Ridge Regression Mean R-Squared Score: 0.7807807487768372\n",
      "Random Forest Regression Mean R-Squared Score: 0.7599016971739884\n",
      "Gradient Boostin Regression Mean R-Squared Score: 0.7382486321363781\n",
      "Decision Tree Regression Mean R-Squared Score: 0.5513520279238323\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Load data\n",
    "admissionData = pd.read_csv('admission_data.csv')\n",
    "admissionData.columns = ['Serial', 'GRE', 'TOFEL', 'Uni-Rate', 'SOP', 'LOR', 'CGPA', 'Research', 'CHofAD']\n",
    "\n",
    "# determine features and target parts in dataset\n",
    "features = admissionData.drop('CHofAD', axis=1)\n",
    "target = admissionData['CHofAD']\n",
    "\n",
    "#Initialize linear regression and Perform 10-fold cross-validation and calculate mean R-Squared score \n",
    "linearReg_model = LinearRegression()\n",
    "linearReg_scores = cross_val_score(linearReg_model, features, target, cv=10, scoring='r2')\n",
    "linear_mean_score = linearReg_scores.mean()\n",
    "print('Linear Regression Mean R-Squared Score:', linear_mean_score)\n",
    "\n",
    "#Initialize rigid regression and Perform 10-fold cross-validation and calculate mean R-Squared score\n",
    "ridgeReg_model = Ridge(alpha=1.0)\n",
    "ridgeReg_scores = cross_val_score(ridgeReg_model, features, target, cv=10, scoring='r2')\n",
    "ridge_mean_score = ridgeReg_scores.mean()\n",
    "print('Ridge Regression Mean R-Squared Score:', ridge_mean_score)\n",
    "\n",
    "#Initialize randomforest regression and Perform 10-fold cross-validation and calculate mean R-Squared score\n",
    "randomForestReg_model = RandomForestRegressor(n_estimators=100)\n",
    "randomForestReg_scores = cross_val_score(randomForestReg_model, features, target, cv=10, scoring='r2')\n",
    "randomForest_mean_score = randomForestReg_scores.mean()\n",
    "print('Random Forest Regression Mean R-Squared Score:', randomForest_mean_score)\n",
    "\n",
    "#Initialize gradient boostin regression and Perform 10-fold cross-validation and calculate mean R-Squared score \n",
    "gradientBoostinReg_model = GradientBoostingRegressor()\n",
    "gradientBoostinReg_scores = cross_val_score(gradientBoostinReg_model, features, target, cv=10, scoring='r2')\n",
    "gradientBoostin_mean_score = gradientBoostinReg_scores.mean()\n",
    "print('Gradient Boostin Regression Mean R-Squared Score:', gradientBoostin_mean_score)\n",
    "\n",
    "#Initialize decision tree regression and Perform 10-fold cross-validation and calculate mean R-Squared score \n",
    "decisionTreeReg_model = DecisionTreeRegressor()\n",
    "decisionTreeReg_scores = cross_val_score(decisionTreeReg_model, features, target, cv=10, scoring='r2')\n",
    "decisionTree_mean_score = decisionTreeReg_scores.mean()\n",
    "print('Decision Tree Regression Mean R-Squared Score:', decisionTree_mean_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Part c)\n",
    "Repeat part b utilizing the Mean Square Error to gauge performance. Briefly research the difference between the two metrics (MSE and R2), and explain in a paragraph or two i. the difference between them ii. when each one is the preferable metric to use. (You can use a cell in a jupyter notebook file for this or a separate text/document file).\n",
    "\n",
    "## Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean MSE Score: 0.004291259223446245\n",
      "Ridge Regression Mean MSE Score: 0.004289860296942606\n",
      "Random Forest Regression Mean MSE Score: 0.004715197299999996\n",
      "Gradient Boostin Regression Mean MSE Score: 0.0053031732190368785\n",
      "Decision Tree Regression Mean MSE Score: 0.008126\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Load data\n",
    "admissionData = pd.read_csv('admission_data.csv')\n",
    "admissionData.columns = ['Serial', 'GRE', 'TOFEL', 'Uni-Rate', 'SOP', 'LOR', 'CGPA', 'Research', 'CHofAD']\n",
    "\n",
    "# determine features and target parts in dataset\n",
    "features = admissionData.drop('CHofAD', axis=1)\n",
    "target = admissionData['CHofAD']\n",
    "\n",
    "\n",
    "#Initialize linear regression and Perform 10-fold cross-validation and calculate mean MSE score \n",
    "linearReg_model = LinearRegression()\n",
    "linearReg_scores = cross_val_score(linearReg_model, features, target, cv=10, scoring='neg_mean_squared_error')\n",
    "linear_mse = -linearReg_scores\n",
    "linear_mean_mse = linear_mse.mean()\n",
    "print('Linear Regression Mean MSE Score:', linear_mean_mse)\n",
    "\n",
    "#Initialize rigid regression and Perform 10-fold cross-validation and calculate mean MSE score\n",
    "ridgeReg_model = Ridge(alpha=1.0)\n",
    "ridgeReg_scores = cross_val_score(ridgeReg_model, features, target, cv=10, scoring='neg_mean_squared_error')\n",
    "ridge_mse = -ridgeReg_scores\n",
    "ridge_mean_mse = ridge_mse.mean()\n",
    "print('Ridge Regression Mean MSE Score:', ridge_mean_mse)\n",
    "\n",
    "#Initialize randomforest regression and Perform 10-fold cross-validation and calculate mean MSE score\n",
    "RandomForestReg_model = RandomForestRegressor(n_estimators=100)\n",
    "RandomForestReg_scores = cross_val_score(RandomForestReg_model, features, target, cv=10, scoring='neg_mean_squared_error')\n",
    "RandomForest_mse = -RandomForestReg_scores\n",
    "RandomForest_mean_mse = RandomForest_mse.mean()\n",
    "print('Random Forest Regression Mean MSE Score:', RandomForest_mean_mse)\n",
    "\n",
    "#Initialize gradient boostin regression and Perform 10-fold cross-validation and calculate mean MSE score\n",
    "gradientBoostinReg_model = GradientBoostingRegressor()\n",
    "gradientBoostinReg_scores = cross_val_score(gradientBoostinReg_model, features, target, cv=10, scoring='neg_mean_squared_error')\n",
    "gradientBoostin_mse = -gradientBoostinReg_scores\n",
    "gradientBoostin_mean_score = gradientBoostin_mse.mean()\n",
    "print('Gradient Boostin Regression Mean MSE Score:', gradientBoostin_mean_score)\n",
    "\n",
    "#Initialize decision tree regression and Perform 10-fold cross-validation and calculate mean MSE score \n",
    "decisionTreeReg_model = DecisionTreeRegressor()\n",
    "decisionTreeReg_scores = cross_val_score(decisionTreeReg_model, features, target, cv=10, scoring='neg_mean_squared_error')\n",
    "decisionTree_mse = -decisionTreeReg_scores\n",
    "decisionTree_mean_score = decisionTree_mse.mean()\n",
    "print('Decision Tree Regression Mean MSE Score:', decisionTree_mean_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE:\n",
    "MSE shows the average squared difference between the actual values and predicted values of the target variable. Infact, it quantifies the overall magnitude of the error of the model. A lower MSE indicates that the model is a better fit for the data. Therefore based on MSE Ridge Regression and Linear Regression are the best models and Decision Tree is the worst model.\n",
    "\n",
    "## R2:\n",
    "R2 shows the proportion of the variation in the target variable that is explained by the features in the model. The value of R2 is between 0 and 1, where higher values indicating that the model explains more of the variation in the target variable. is useful when the focus is on understanding the relationship between the dependent and independent variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
